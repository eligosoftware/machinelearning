y
x <- seq(0, 1, by = .0001)
y <- dnorm(x, mean = 0.5, sd = 1/7)
plot(x,y)
y
y <- dnorm(x, mean = 0.5, sd = 0.1)
plot(x,y)
y
pnorm(0, lower.tail = FALSE) - pnorm(1.37, lower.tail = FALSE)
pnorm(0, lower.tail = FALSE) - pnorm(1.37, lower.tail = FALSE)
pnorm(0.5, lower.tail = FALSE) - pnorm(1.37, lower.tail = FALSE)
y
x <- seq(0, 1, by = .0001)
y <- dnorm(x, mean = 0.5, sd = 0.1)
plot(x,y)
y
y <- dnorm(x, mean = 0.5, sd = 10)
plot(x,y)
y
y <- dnorm(x, mean = 0.5, sd = 5)
plot(x,y)
y
y <- dnorm(x, mean = 0.5, sd = 0.1)
plot(x,y)
y
y <- dnorm(x, mean = 0.5, sd = 0.2)
plot(x,y)
y
pnorm(1, mean=0.5, sd=0.2, lower.tail=FALSE)
perc<-pnorm(1, mean=0.5, sd=0.2, lower.tail=FALSE)
10000*perc
ans<-10000*perc
ans
function (x, mean = 0, sd = 1, log = FALSE)
y <- pnorm(x, mean = 0.5, sd = 0.2)
plot(x,y)
y
perc<-pnorm(1, mean=0.5, sd=0.2, lower.tail=FALSE)
r.tail=FALSE)
r.tail=FALSE)
perc<-pnorm(1, mean=0.5, sd=0.2, lower.tail=FALSE)
ans<-10000*perc
ans
perc<-pnorm(x, mean=0.5, sd=0.2, lower.tail=FALSE)
ans<-10000*perc
ans
y <- dnorm(x, mean = 0.5, sd = 0.2)
plot(x,y)
y
perc<-pnorm(1, mean=0.5, sd=0.2, lower.tail=FALSE)
ans<-10000*perc
ans
perc<-pnorm(0.8, mean=0.5, sd=0.2, lower.tail=FALSE)
ans<-10000*perc
ans
# sequence of 10000 x from 0 to 1 with step 0.0001
x <- seq(0, 10000, by = 1)
# normal distribution with mean equals 0.5 and standart deviation 0.2 , 5 sigma
y <- dnorm(x, mean = 0.5, sd = 0.2)
# plot x and y
plot(x,y)
y
#find percentage of points higher 5 sigma from 0 (more than 1)
perc<-pnorm(1, mean=0.5, sd=0.2, lower.tail=FALSE)
ans<-10000*perc
#find the amount of points, multiply percentage to total amount
ans<-10000*perc
ans
# sequence of 10000 x from 0 to 1 with step 0.0001
x <- seq(0, 1, by = .0001)
# normal distribution with mean equals 0.5 and standart deviation 0.2 , 5 sigma
y <- dnorm(x, mean = 0.5, sd = 0.2)
# plot x and y
plot(x,y)
#find percentage of points higher 5 sigma from 0 (more than 1)
perc<-pnorm(1, mean=0.5, sd=0.2, lower.tail=FALSE)
#find the amount of points, multiply percentage to total amount
ans<-10000*perc
ans
source('C:/Users/User/Downloads/pset7/ex2_r.R')
# import library
library("deSolve")
#differential functions
sir_equations <- function(time, variables, parameters) {
with(as.list(c(variables, parameters)), {
dS <- -alpha * I * S
dI <-  alpha * I * S - beta * I
dR <-  beta * I
return(list(c(dS, dI, dR)))
})
}
#parameters
parameters_values <- c(
alpha  = 0.01, # infectious contact rate (/person/day)
beta = 0.8    # recovery rate (/day)
)
initial_values <- c(
S = 999,  # number of susceptibles at time = 0
I =   1,  # number of infectious at time = 0
R =   0   # number of recovered (and immune) at time = 0
)
# solution
sir_values_1 <- ode(
y = initial_values,
times = time_values,
func = sir_equations,
parms = parameters_values
)
sir_values_1
sir_values_1 <- as.data.frame(sir_values_1)
sir_values_1
with(sir_values_1, {
# plotting the time series of susceptibles:
plot(time, S, type = "l", col = "blue",
xlab = "time (days)", ylab = "number of people")
# adding the time series of infectious:
lines(time, I, col = "red")
# adding the time series of recovered:
lines(time, R, col = "green")
})
# adding a legend:
legend("right", c("susceptibles", "infectious", "recovered"),
col = c("blue", "red", "green"), lty = 1, bty = "n")
install.packages("ISLR")
install.packages("ISLR")
install.packages("ISLR")
library(ISLR)
fix(Boston)
names(Boston)
fix(Boston)
names(Boston)
Boston
library(MASS)
fix(Boston)
names(Boston)
Boston
data("Boston")
fix(Boston)
names(Boston)
lm.fit=lm(medv~lstat)
lm.fit=lm(medv~lstat,data=Boston)
attach(Boston)
lm.fit=lm(medv~lstat)
lm.fit
summary(lm.fit)
predict (lm.fit ,data.frame(lstat=c(5 ,10 ,15) ),
interval =" confidence ")
predict (lm.fit ,data.frame(lstat=c(5 ,10 ,15) ),interval =" confidence ")
predict (lm.fit ,data.frame(lstat=c(5 ,10 ,15) ),interval ="confidence")
plot(lstat,medv)
abline(lm.fit)
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3,pch="20")
abline(lm.fit,lwd=3,pch="20")
abline(lm.fit,lwd=3,pch=20)
abline(lm.fit,lwd=3,pch="+")
abline(lm.fit,pch="+")
par(mfrow=c(2,2))
par(mfrow=c(2,2))
plot(Lm.fit)
plot(lm.fit)
library(tidyverse)
library(help="datasets")
objects()
is()
(x<-1.2:6.4)
x*2
x/2
x-1
x-1
(x<-c(12,16,22,23,34,3,8))
(f<-(x*9/5)+32)
x<-15:30
x[c(2,5)]
c(x[2],x[5])
(x<-c(10,20,NA,4,NA,2))
sum(x)/length(x)
i<-is.na(x)
i
(y<-x[!i])
n<-length(x[!i]);new.x<-x[!i]; sum(new.x) /n
mean(x)
mean(x,na.rm = TRUE)
fruit<-c(5,10,1,20)
names(fruit)<-c("orange","banana","apple","peach")
fruit[c("apple","orange")]
dev.new()
x<-0:19
y<-x+runif(20,min = 10,max = 20)
y
plot(x,y)
typeof()
x <- list(list(1, 2), c(3, 4))
x
x
x
str(x)
y <- c(list(1, 2), c(3, 4))
str(y)
read.table(file.choose(), header=TRUE)
#----------------------------------------------------------
# run the first set with 100 trials
#----------------------------------------------------------
rm(list=ls())
# prepare the graphic window for the four outputs
parBackup <- par()
par(mfrow=c(2,2))
# run a loop 4 times to simulate the experiment with 100 trials 4 times
for (v in 1:4)
{
# run the experiment 100 times (= 100 flips) and record the results (success = 1, failure = 0)
# We can choose if success represesnt tails or heads. Here, for the rest of the process
# we consider success to represent tails.
records <- rbinom(100,1,0.5)
# determine the cummulative number of heads after each flip
cummulativeRecords <- cumsum(records)
# determine the proportion of heads after each flip
cummulativeProportion <- cummulativeRecords/c(1:length(records))
# plot the proportion of heads against the number of trials
plot(c(1:length(records)), cummulativeProportion, xlab="Number of Trials", ylab= "Proportion of Heads", ylim=c(0,1))
# add a line that represents the eassumed probability (= proportion) of heads
# to show up after a lot of flips
abline(0.5, 0)
# add some text, that displays the final proportion
t <- paste("Final Proportion: ", cummulativeProportion[length(cummulativeProportion)])
text(60, 0.8, t)
}
# reset the graphics settings
par(parBackup)
#----------------------------------------------------------
# run the same with 1000 trials
#----------------------------------------------------------
rm(list=ls())
# prepare the graphic window for the four outputs
parBackup <- par()
par(mfrow=c(2,2))
# run a loop 4 times to simulate the experiment with 1000 trials (=flips) 4 times
for (v in 1:4)
{
records <- rbinom(1000,1,0.5)
cummulativeRecords <- cumsum(records)
cummulativeProportion <- cummulativeRecords/c(1:length(records))
plot(c(1:length(records)), cummulativeProportion, xlab="Number of Trials", ylab= "Proportion of Heads", ylim=c(0,1))
abline(0.5, 0)
t <- paste("Final Proportion: ", cummulativeProportion[length(cummulativeProportion)])
text(600, 0.8, t)
}
par(parBackup)
#----------------------------------------------------------
# run the first set with 100 trials
#----------------------------------------------------------
rm(list=ls())
# prepare the graphic window for the four outputs
parBackup <- par()
par(mfrow=c(2,2))
# run a loop 4 times to simulate the experiment with 100 trials 4 times
for (v in 1:4)
{
# run the experiment 100 times (= 100 flips) and record the results (success = 1, failure = 0)
# We can choose if success represesnt tails or heads. Here, for the rest of the process
# we consider success to represent tails.
records <- rbinom(100,1,0.5)
# determine the cummulative number of heads after each flip
cummulativeRecords <- cumsum(records)
# determine the proportion of heads after each flip
cummulativeProportion <- cummulativeRecords/c(1:length(records))
# plot the proportion of heads against the number of trials
plot(c(1:length(records)), cummulativeProportion, xlab="Number of Trials", ylab= "Proportion of Heads", ylim=c(0,1))
# add a line that represents the eassumed probability (= proportion) of heads
# to show up after a lot of flips
abline(0.5, 0)
# add some text, that displays the final proportion
t <- paste("Final Proportion: ", cummulativeProportion[length(cummulativeProportion)])
text(60, 0.8, t)
}
# reset the graphics settings
par(parBackup)
#----------------------------------------------------------
# run the same with 1000 trials
#----------------------------------------------------------
rm(list=ls())
# prepare the graphic window for the four outputs
parBackup <- par()
par(mfrow=c(2,2))
# run a loop 4 times to simulate the experiment with 1000 trials (=flips) 4 times
for (v in 1:4)
{
records <- rbinom(1000,1,0.5)
cummulativeRecords <- cumsum(records)
cummulativeProportion <- cummulativeRecords/c(1:length(records))
plot(c(1:length(records)), cummulativeProportion, xlab="Number of Trials", ylab= "Proportion of Heads", ylim=c(0,1))
abline(0.5, 0)
t <- paste("Final Proportion: ", cummulativeProportion[length(cummulativeProportion)])
text(600, 0.8, t)
}
#----------------------------------------------------------
# Reset R's brain
#----------------------------------------------------------
rm(list=ls())
#----------------------------------------------------------
# Reset graphic device
# As long as there is any dev open (exept "null device")
# close the active one!
# Caution: closes all open plots!!!!
#----------------------------------------------------------
while(!is.null(dev.list()))
{
dev.off()
}
#----------------------------------------------------------
# Tell R where to find our data
#----------------------------------------------------------
getwd()
setwd("C:/Users/RD/sciebo/stat/Master")
getwd()
x <- seq(-4, 4, length=100)
hx <- dnorm(x)
degf <- c(1, 3, 8, 30)
colors <- c("red", "blue", "darkgreen", "gold", "black")
labels <- c("df=1", "df=3", "df=8", "df=30", "normal")
plot(x, hx, type="l", lty=2, xlab="x value",
ylab="Density", main="Comparison of t Distributions")
for (i in 1:4){
lines(x, dt(x,degf[i]), lwd=2, col=colors[i])
}
legend("topright", inset=.05, title="Distributions",
labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)
mean=100; sd=15
lb=80; ub=120
x <- seq(-4,4,length=100)*sd + mean
hx <- dnorm(x,mean,sd)
plot(x, hx, type="n", xlab="IQ Values", ylab="",
main="Normal Distribution", axes=FALSE)
i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="red")
area <- pnorm(ub, mean, sd) - pnorm(lb, mean, sd)
result <- paste("P(",lb,"< IQ <",ub,") =",
signif(area, digits=3))
mtext(result,3)
axis(1, at=seq(40, 160, 20), pos=0)
#----------
# Example 3
#----------
#----------------------------------------------------------
# Cumulative Sum
#----------------------------------------------------------
m<-c(20:40) # egg cluster sizes to be examined
pm<-c(rep(0,6),1/30*((26:30)-25),(36-((31:35)))/30,rep(0,5)) # PMF
cm<-cumsum(pm) # CDF
barplot(pm, xlab="Cluster Size",, ylab="Probability",names.arg=m, main="CDF")
barplot(cm, xlab="Cluster Size",, ylab="Probability",names.arg=m, main="CDF")
cmEasy<-cumsum(m)
m
cmEasy
barplot(cmEasy, xlab="Cluster Size")
#----------
# Example 4
#----------
#----------------------------------------------------------
# According to a study 86% of the countries households
# own a cellular phone (one or more).
# In a random sample of 20 households, what is the
# probability that exactly 15 own a cell phone?
#----------------------------------------------------------
dbinom(15,20,0.86)
#----------
# Example 5
#----------
#----------------------------------------------------------
# In a random sample of 20 households, what is the
# probability that the number of households owning a
# cell phone is between 15 and 17?
#----------------------------------------------------------
dbinom(15,20,0.86)+dbinom(16,20,0.86)+dbinom(17,20,0.86)
pbinom(17, size=20, prob=0.86)-pbinom(14, size=20, prob=0.86)
n=100
p=0.5
# plots a Binomial probability  distribution
k <- 0:n
prob <- dbinom(k, n, p)
names(prob) <- as.character(0:n)
barplot(prob,space=0)
rbinom(10,100,0.5)
# To use the hist.binom function, you must specify the values of n and p.
# For example, to get the histogram of a binomial(6,1/3) distribution, use
# hist.binom(6,1/3)
# Try experimenting with the color ("col") argument; use help(colors) or colors()
# to see what colors R knows by name.
hist.binom <- function(n, p, ...)
{
# plots a Binomial probability  distribution
k <- 0:n
p <- dbinom(k, n, p)
names(p) <- as.character(0:n)
barplot(p,space=0, ...)
}
par(mfrow=c(1,3)) # Splits graphics output into a table of  3 columns
hist.binom(10,0.2,col="yellow", main="Binom. Dist. with n=10 and p=0.2", xlab="Number of Successes", ylab="Probability")
hist.binom(10,0.5,col="yellow", main="Binom. Dist. with n=10 and p=0.5", xlab="Number of Successes")
hist.binom(10,0.8,col="yellow", main="Binom. Dist. with n=10 and p=0.8", xlab="Number of Successes")
par(mfrow=c(1,1)) # back to default
# Alternative ways to plot a Binomial probability  distribution
rm(list=ls())
n<-300
k <- 0:n
probs <- dbinom(k, n, 0.2)
names(probs) <- as.character(0:n)
probs
plot(k,probs, type="h", ylim=c(0,max(probs)))
plot(k,probs, type="s", ylim=c(0,max(probs)))
plot(k,probs, type="h", ylim=c(0,max(probs)), xlim=c(40,80))
# Reset graphic device
while(!is.null(dev.list()))
{
dev.off()
}
par(mar=c(4,5,8,1))
par(mfrow=c(1,3)) # Splits graphics output into a table of  3 columns
hist.binom(10,0.1,col="yellow", main="p=0.1 and n=10",xlim= c(0,7), xlab="Number of Successes", ylab="Probability")
hist.binom(30,0.1,col="yellow", main="p=0.1 and n=30",xlim= c(0,12),xlab="Number of Successes")
setwd("~/machine learning/Deep Learning/Artificial Neural Networks/R")
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
View(dataset)
View(dataset)
dataset=dataset[,3:5]
View(before_support)
View(dataset)
View(dataset)
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset=dataset[,4:8]
View(dataset)
View(dataset)
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset=dataset[,4:-1]
dataset=dataset[,4:10]
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset=dataset[,4:11]
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset=dataset[,4:12]
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset=dataset[,4:13]
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset=dataset[,4:13]
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset=dataset[,4:14]
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset=dataset[,4:14]
dataset=dataset[4:14]
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset=dataset[4:14]
dataset$Geography=factor(dataset$Geography,levels=c('France','Spain','Germany'),
labels = c(1,2,3))
# Importing the dataset
dataset = read.csv('Churn_Modelling.csv')
dataset=dataset[4:14]
# we still need to encode geography and gender
dataset$Geography=as.numeric(factor(dataset$Geography,levels=c('France','Spain','Germany'),
labels = c(1,2,3)))
dataset$Gender=as.numeric(factor(dataset$Gender,levels=c('Female','Male'),
labels = c(1,2)))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
library(caTools)
set.seed(123)
split = sample.split(dataset$Exited, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
View(training_set)
View(training_set)
# Feature Scaling
training_set[-11] = scale(training_set[-11])
test_set[-11] = scale(test_set[-11])
View(training_set)
# Fitting ANN to the Training data
install.packages('h2o')
library(h2o)
h2o.init(nthreads = -1)  # -1 - all available cores of processor for computation
classifier=h2o.deeplearning(y='Exited',training_frame = as.h2o(training_set),
activation = 'Rectifier',
hidden=c(6,6),
epochs = 100,
train_samples_per_iteration = -2)
# Predicting the Test set results
prob_pred=h2o.predict(classifier,newdata= as.h2o(test_set[-11])
# Predicting the Test set results
prob_pred=h2o.predict(classifier,newdata= as.h2o(test_set[-11]))
prob_pred
# Predicting the Test set results
prob_pred=h2o.predict(classifier,newdata= as.h2o(test_set[-11]))
prob_pred
y_pred=(prob_pred>0.5)
y_pred=as.vector(y_pred)
y_pred
cn=table(test_set[,11],y_pred)
cn
cn
(1530+196)/(211+63)
(1530+196)/(211+63+1530+196)
# disconnect from h2o server
h2o.shutdown()
